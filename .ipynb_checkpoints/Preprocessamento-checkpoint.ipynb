{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b486c2a-4428-4f13-8e94-cd896eb36731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaForQuestionAnswering, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc03d2c4-5572-4be3-a82b-b80462e355fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "model = RobertaForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66a91ab4-0db7-4b50-aa2c-180241896c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 124056578\n",
      "all model parameters: 124056578\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9b8381c5-d5de-405f-bc09-473b00f701f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdados\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "with open(\"Dados/dataset_coleta_SEDS.json\", 'r', encoding = 'utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "len(data)\n",
    "data['dados']['question']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9c03fd4d-0023-41f1-a0b8-7898c60e9387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "for item in data['dados']:\n",
    "    context = item['context']\n",
    "    question = item['question']\n",
    "    answers = item['answers']\n",
    "    \n",
    "    # Iterar sobre as respostas para extrair o text e answer_start\n",
    "    for answer in answers:\n",
    "        text = answer['text']\n",
    "        answer_start = answer['answer_start']\n",
    "        \n",
    "        # Adicionar os dados extraídos à lista data_list\n",
    "    data_list.append({'context': context, 'question': question, 'answer': {'text': text, 'answer_start': answer_start}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267820c6-5ec6-49f9-bab4-93d325585b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for programa, lista_perguntas_respostas in data.items():\n",
    "    # Criar uma entrada para o programa no dicionário\n",
    "    dataset_por_programa[programa] = []\n",
    "    # Percorrer cada item (dicionário) etiquetar prompts e completions\n",
    "    for item in lista_perguntas_respostas:\n",
    "        dataset_por_programa[programa].append({'prompt': f\"{programa} : {item['prompt']}\", 'completion': item['completion']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0e49aa9e-50fc-47a6-987f-69dd185d0304",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'answers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDados/dataset_coleta_SEDS.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'answers'"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"Dados/dataset_coleta_SEDS.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "de60b97c-af39-4841-bb6e-cc8fd5919277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data_list, test_size = 0.25, random_state = 42)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2e8abe46-d3a6-4a65-a7d3-fc1c0fdac621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho máximo da pergunta: 1\n",
      "Tamanho máximo do contexto: 1\n",
      "Tamanho máximo da pergunta: 1\n",
      "Tamanho máximo do contexto: 1\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Carregar o tokenizer do modelo RoBERTa pré-treinado\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Função para pré-processar os dados\n",
    "def preprocess_data(data):\n",
    "    processed_data = []\n",
    "    max_question_len = 0\n",
    "    max_context_len = 0\n",
    "    \n",
    "    for item in data:\n",
    "        # Tokenizar a pergunta e o contexto\n",
    "        question_tokens = tokenizer(item[\"question\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64)\n",
    "        context_tokens = tokenizer(item[\"context\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256)\n",
    "        \n",
    "        # Salvar o tamanho máximo da pergunta e do contexto\n",
    "        max_question_len = max(max_question_len, len(question_tokens[\"input_ids\"]))\n",
    "        max_context_len = max(max_context_len, len(context_tokens[\"input_ids\"]))\n",
    "        \n",
    "        # Adicionar os tokens tokenizados e as respostas ao conjunto de dados processado\n",
    "        processed_data.append({\n",
    "            \"input_ids\": question_tokens[\"input_ids\"],\n",
    "            \"attention_mask\": question_tokens[\"attention_mask\"],\n",
    "            \"context_input_ids\": context_tokens[\"input_ids\"],\n",
    "            \"context_attention_mask\": context_tokens[\"attention_mask\"],\n",
    "            \"answer\": item[\"answer\"]\n",
    "        })\n",
    "    \n",
    "    print(\"Tamanho máximo da pergunta:\", max_question_len)\n",
    "    print(\"Tamanho máximo do contexto:\", max_context_len)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "processed_train_data = preprocess_data(train_data)\n",
    "processed_test_data = preprocess_data(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "43d901ee-da5e-4e42-beb5-811138a2988c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Context of the Question.\n",
      "\n",
      "O Aprendiz do Futuro é uma iniciativa do Governo do Estado de Goiás, por meio da Secretaria de Estado de Desenvolvimento Social (SEDS), que tem como visão oferecer o maior programa de sócio-aprendizagem do Brasil e do mundo com foco em tecnologia, alta performance e impacto social na vida dos jovens em situação de vulnerabilidade.\n",
      "\n",
      "\n",
      "Question:\n",
      "Como o Programa Aprendiz do Futuro contribui para o desenvolvimento pessoal e profissional dos jovens participantes?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL ANSWER - ZERO SHOT:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 5\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context of the Question.\n",
    "\n",
    "{train_data[index]['context']}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(train_data[index]['question'], train_data[index]['context'], return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "resposta = tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(f'Question:\\n{train_data[index]['question']}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL ANSWER - ZERO SHOT:\\n{resposta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "748e26ed-e704-46a9-b0a7-23dbc69df120",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.03\n"
     ]
    }
   ],
   "source": [
    "start_probs = torch.nn.functional.softmax(outputs.start_logits, dim=1)\n",
    "end_probs = torch.nn.functional.softmax(outputs.end_logits, dim=1)\n",
    "\n",
    "start_one_hot = torch.nn.functional.one_hot(torch.tensor([answer_start_index]), num_classes=start_probs.size(1))\n",
    "end_one_hot = torch.nn.functional.one_hot(torch.tensor([answer_end_index]), num_classes=end_probs.size(1))\n",
    "\n",
    "start_loss = torch.nn.functional.binary_cross_entropy(start_probs, start_one_hot.float())\n",
    "end_loss = torch.nn.functional.binary_cross_entropy(end_probs, end_one_hot.float())\n",
    "\n",
    "total_loss = start_loss + end_loss\n",
    "\n",
    "total_loss_value = total_loss.item()\n",
    "\n",
    "rounded_loss = round(total_loss_value, 2)\n",
    "\n",
    "print(f'Loss: {rounded_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cf1d1e18-2256-4ec4-8f47-99b937d8176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Answer the question.\\n\\n'\n",
    "    end_prompt = '\\n\\nAnswer: '\n",
    "    prompt = start_prompt + example[\"question\"] + end_prompt\n",
    "    \n",
    "    # Tokenizar a pergunta e o contexto\n",
    "    inputs = tokenizer(prompt, example[\"context\"],\n",
    "                       add_special_tokens=True, padding='max_length', truncation=True,\n",
    "                       max_length=1000, return_tensors=\"pt\")\n",
    "\n",
    "    # Adicionar os inputs ao exemplo\n",
    "    example['input_ids'] = inputs.input_ids\n",
    "    example['attention_mask'] = inputs.attention_mask\n",
    "    \n",
    "    # Encontrar as posições de início e fim das respostas\n",
    "    start_pos = example[\"context\"].find(example[\"answer\"])\n",
    "    if start_pos == -1:\n",
    "        # Se a resposta não está presente no contexto, atribuir posição 0\n",
    "        example['start_positions'] = [0]\n",
    "        example['end_positions'] = [0]\n",
    "    else:\n",
    "        # Se a resposta está presente no contexto, atribuir posições de início e fim\n",
    "        end_pos = start_pos + len(example[\"answer\"]) - 1\n",
    "        example['start_positions'] = [start_pos]\n",
    "        example['end_positions'] = [end_pos]\n",
    "    \n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "da99ad74-944b-4075-bbe9-b2e7d88c9090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No caso de perda ou extravio do cartão, entrar em contato com a BK Bank pelo número 08009010203 ou pelo Whatsapp (16) 99375-7412.', 'Para receber os benefícios não é necessário realizar inscrição, pois o Governo de Goiás usa a base de dados do Cadastro Único (CadÚnico).', 'O Aprendiz do Futuro é uma iniciativa do Governo do Estado de Goiás, por meio da Secretaria de Estado de Desenvolvimento Social (SEDS), que tem como visão oferecer o maior programa de sócio-aprendizagem do Brasil e do mundo com foco em tecnologia, alta performance e impacto social na vida dos jovens em situação de vulnerabilidade.', 'O Dignidade beneficia com R$ 300 mensais pessoas que tenham entre 60 anos e 64 anos 11 meses e 29 dias em situação de pobreza ou de extrema pobreza.']\n"
     ]
    }
   ],
   "source": [
    "print(test_data['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "515fb2e6-3312-48c5-a46d-200a99ee4b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para armazenar os exemplos tokenizados\n",
    "tokenized_test_data = []\n",
    "\n",
    "# Itera sobre cada exemplo no conjunto de dados de teste\n",
    "for example in test_data:\n",
    "    # Aplica a função tokenize_function ao exemplo atual\n",
    "    tokenized_example = tokenize_function(example)\n",
    "    # Adiciona o exemplo tokenizado à lista\n",
    "    tokenized_test_data.append(tokenized_example)\n",
    "\n",
    "\n",
    "# Lista para armazenar os exemplos tokenizados\n",
    "tokenized_train_data = []\n",
    "\n",
    "# Itera sobre cada exemplo no conjunto de dados de teste\n",
    "for example in train_data:\n",
    "    # Aplica a função tokenize_function ao exemplo atual\n",
    "    tokenized_example = tokenize_function(example)\n",
    "    # Adiciona o exemplo tokenizado à lista\n",
    "    tokenized_train_data.append(tokenized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4155cded-c0b8-4ad1-bef4-ff390c76fe9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do tensor 0 do dataset tokenizado (treinamento):\n",
      "input_ids: torch.Size([70, 1000])\n",
      "attention_mask: torch.Size([70, 1000])\n",
      "Tamanho do tensor 1 do dataset tokenizado (treinamento):\n",
      "input_ids: torch.Size([45, 1000])\n",
      "attention_mask: torch.Size([45, 1000])\n",
      "Tamanho do tensor 2 do dataset tokenizado (treinamento):\n",
      "input_ids: torch.Size([54, 1000])\n",
      "attention_mask: torch.Size([54, 1000])\n",
      "Tamanho do tensor 3 do dataset tokenizado (treinamento):\n",
      "input_ids: torch.Size([63, 1000])\n",
      "attention_mask: torch.Size([63, 1000])\n",
      "Tamanho do tensor 4 do dataset tokenizado (treinamento):\n",
      "input_ids: torch.Size([52, 1000])\n",
      "attention_mask: torch.Size([52, 1000])\n",
      "Tamanho do tensor 5 do dataset tokenizado (treinamento):\n",
      "input_ids: torch.Size([116, 1000])\n",
      "attention_mask: torch.Size([116, 1000])\n",
      "Tamanho do tensor 6 do dataset tokenizado (treinamento):\n",
      "input_ids: torch.Size([43, 1000])\n",
      "attention_mask: torch.Size([43, 1000])\n",
      "Tamanho do tensor 7 do dataset tokenizado (treinamento):\n",
      "input_ids: torch.Size([45, 1000])\n",
      "attention_mask: torch.Size([45, 1000])\n",
      "Tamanho do tensor 8 do dataset tokenizado (treinamento):\n",
      "input_ids: torch.Size([55, 1000])\n",
      "attention_mask: torch.Size([55, 1000])\n",
      "Tamanho do tensor 0 do dataset tokenizado (teste):\n",
      "input_ids: torch.Size([24, 1000])\n",
      "attention_mask: torch.Size([24, 1000])\n",
      "Tamanho do tensor 1 do dataset tokenizado (teste):\n",
      "input_ids: torch.Size([38, 1000])\n",
      "attention_mask: torch.Size([38, 1000])\n",
      "Tamanho do tensor 2 do dataset tokenizado (teste):\n",
      "input_ids: torch.Size([38, 1000])\n",
      "attention_mask: torch.Size([38, 1000])\n",
      "Tamanho do tensor 3 do dataset tokenizado (teste):\n",
      "input_ids: torch.Size([29, 1000])\n",
      "attention_mask: torch.Size([29, 1000])\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate(tokenized_train_data):\n",
    "    input_ids_size = example['input_ids'].size()\n",
    "    attention_mask_size = example['attention_mask'].size()\n",
    "    print(f\"Tamanho do tensor {i} do dataset tokenizado (treinamento):\")\n",
    "    print(\"input_ids:\", input_ids_size)\n",
    "    print(\"attention_mask:\", attention_mask_size)\n",
    "\n",
    "for i, example in enumerate(tokenized_test_data):\n",
    "    input_ids_size = example['input_ids'].size()\n",
    "    attention_mask_size = example['attention_mask'].size()\n",
    "    print(f\"Tamanho do tensor {i} do dataset tokenizado (teste):\")\n",
    "    print(\"input_ids:\", input_ids_size)\n",
    "    print(\"attention_mask:\", attention_mask_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "42b663bf-c773-44c4-8efb-b515b4f9c7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator, DataLoaderConfiguration\n",
    "\n",
    "dataloader_config = DataLoaderConfiguration(\n",
    "    dispatch_batches=None,\n",
    "    split_batches=False,\n",
    "    even_batches=True,\n",
    "    use_seedable_sampler=True\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(dataloader_config=dataloader_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3ea96d89-51a0-4106-9462-2dd63f1740f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,  # Altere para o seu modelo original\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_train_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5933ebd4-0997-4ffd-8ca0-a4bcabf764d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    # Separe as entradas e os rótulos\n",
    "    inputs = [item['input_ids'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    # Obtenha o comprimento máximo da sequência no lote\n",
    "    max_len = max(len(seq) for seq in inputs)\n",
    "    \n",
    "    # Preencha as sequências para que todas tenham o mesmo comprimento\n",
    "    padded_inputs = [seq + [0] * (max_len - len(seq)) for seq in inputs]\n",
    "    \n",
    "    return {\n",
    "        'input_ids': torch.tensor(padded_inputs),\n",
    "        'labels': torch.tensor(labels)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6e67c7ec-0828-478f-af91-8bcea43b4a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 8  # ajuste conforme necessário\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=my_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b05fb303-4a87-471f-8f18-bdc8eab72b16",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [43, 1000] at entry 0 and [52, 1000] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[153], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2085\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2082\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2084\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2085\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\data_loader.py:452\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 452\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer_utils.py:787\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[\u001b[38;5;28mdict\u001b[39m]):\n\u001b[0;32m    786\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_columns(feature) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:92\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[1;34m(features, return_tensors)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# have the same attributes.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# on the whole batch.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_default_data_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:154\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 154\u001b[0m         batch[k] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    156\u001b[0m         batch[k] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mstack([f[k] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [43, 1000] at entry 0 and [52, 1000] at entry 1"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6a2fb8-c06e-45e1-9fa6-4f43d450f67a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f02759-1dae-428d-ba95-6ef6aad119e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
